import logging
import time
import base64
from typing import List, Dict, Any
import requests
import json

from .base_llm_provider import BaseLLMProvider, LLMChunk, LLMResponse

# Get a logger for the current module
logger = logging.getLogger(__name__)

class GeminiProvider(BaseLLMProvider):
    """
    Google Gemini LLM provider implementation.
    
    Supports multimodal processing including text, tables, and images.
    """

    def __init__(self, api_key: str, model_name: str = "gemini-1.5-flash", 
                 rate_limit_delay: float = 2.0, temperature: float = 0.1, max_output_tokens: int = 1024):
        """
        Initialize the Gemini provider.

        Args:
            api_key (str): Google AI API key
            model_name (str): Model name (default: gemini-1.5-flash)
            rate_limit_delay (float): Delay between requests for rate limiting
            temperature (float): Temperature for response generation (0.0-2.0)
            max_output_tokens (int): Maximum output tokens per response
        """
        super().__init__(api_key, model_name, rate_limit_delay)
        self.base_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent"
        
        # Store generation parameters
        self.temperature = temperature
        self.max_output_tokens = max_output_tokens
        
        logger.info(f"GeminiProvider initialized with model: {model_name} (Temp: {temperature}, Max tokens: {max_output_tokens})")

    def _get_supported_modalities(self) -> List[str]:
        """
        Gemini supports full multimodal processing.

        Returns:
            List[str]: Supported chunk types
        """
        return ['text', 'table', 'image']

    def generate_summary(self, chunk: LLMChunk, prompt: str) -> LLMResponse:
        """
        Generate a summary for a single chunk using Gemini API.

        Args:
            chunk (LLMChunk): The chunk to summarize
            prompt (str): The prompt template for summarization

        Returns:
            LLMResponse: The generated summary response
        """
        if not self._validate_chunks([chunk]):
            raise ValueError("Invalid chunk provided")

        self._rate_limit()
        
        start_time = time.time()
        
        try:
            contents = self._prepare_contents([chunk], prompt)
            
            payload = {
                "contents": contents,
                "generationConfig": {
                    "temperature": 0.3,
                    "topK": 40,
                    "topP": 0.95,
                    "maxOutputTokens": 512,
                }
            }
            
            logger.debug(f"Sending summary request to Gemini for chunk type: {chunk.type}")
            
            response = requests.post(
                f"{self.base_url}?key={self.api_key}",
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=30
            )
            
            response.raise_for_status()
            response_data = response.json()
            
            if 'candidates' not in response_data or not response_data['candidates']:
                raise ValueError("No response generated by Gemini")
            
            content = response_data['candidates'][0]['content']['parts'][0]['text']
            tokens_used = response_data.get('usageMetadata', {}).get('totalTokenCount', 0)
            
            response_time = time.time() - start_time
            
            logger.info(f"Successfully generated summary using Gemini (tokens: {tokens_used}, time: {response_time:.2f}s)")
            
            return LLMResponse(
                content=content,
                model_used=self.model_name,
                tokens_used=tokens_used,
                response_time=response_time,
                metadata={"provider": "gemini"}
            )
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Gemini API request failed: {e}")
            raise
        except Exception as e:
            logger.error(f"Error generating summary with Gemini: {e}")
            raise

    def answer_query(self, chunks: List[LLMChunk], prompt: str) -> LLMResponse:
        """
        Answer a user query based on multiple chunks using Gemini API.

        Args:
            chunks (List[LLMChunk]): List of relevant chunks
            prompt (str): The query prompt

        Returns:
            LLMResponse: The generated answer response
        """
        if not chunks:
            raise ValueError("No chunks provided")
        
        if not self._validate_chunks(chunks):
            raise ValueError("Invalid chunks provided")

        self._rate_limit()
        
        start_time = time.time()
        
        try:
            contents = self._prepare_contents(chunks, prompt)
            
            payload = {
                "contents": contents,
                "generationConfig": {
                    "temperature": self.temperature,
                    "topK": 40,
                    "topP": 0.95,
                    "maxOutputTokens": self.max_output_tokens,
                }
            }
            
            logger.debug(f"Sending query request to Gemini with {len(chunks)} chunks")
            
            response = requests.post(
                f"{self.base_url}?key={self.api_key}",
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=30
            )
            
            response.raise_for_status()
            response_data = response.json()
            
            if 'candidates' not in response_data or not response_data['candidates']:
                raise ValueError("No response generated by Gemini")
            
            content = response_data['candidates'][0]['content']['parts'][0]['text']
            tokens_used = response_data.get('usageMetadata', {}).get('totalTokenCount', 0)
            
            response_time = time.time() - start_time
            
            logger.info(f"Successfully answered query using Gemini (tokens: {tokens_used}, time: {response_time:.2f}s)")
            
            return LLMResponse(
                content=content,
                model_used=self.model_name,
                tokens_used=tokens_used,
                response_time=response_time,
                metadata={"provider": "gemini", "chunks_processed": len(chunks)}
            )
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Gemini API request failed: {e}")
            raise
        except Exception as e:
            logger.error(f"Error answering query with Gemini: {e}")
            raise

    def _prepare_contents(self, chunks: List[LLMChunk], prompt: str) -> List[Dict[str, Any]]:
        """
        Prepare contents for Gemini API format.

        Args:
            chunks (List[LLMChunk]): List of chunks to include
            prompt (str): The prompt text

        Returns:
            List[Dict[str, Any]]: Contents formatted for Gemini API
        """
        parts = []
        
        # Add the prompt as text
        parts.append({"text": prompt})
        
        # Add chunks based on their type
        for i, chunk in enumerate(chunks):
            if chunk.type == 'text':
                parts.append({"text": f"\n\nTEXT CHUNK {i+1}:\n{chunk.content}"})
            
            elif chunk.type == 'table':
                parts.append({"text": f"\n\nTABLE {i+1}:\n{chunk.content}"})
            
            elif chunk.type == 'image':
                # Add image data for Gemini
                try:
                    # Validate base64 image data
                    image_data = chunk.content
                    if not self._is_valid_base64_image(image_data):
                        # Fallback to text description if image is invalid
                        description = chunk.metadata.get('caption', 'Invalid image data')
                        parts.append({"text": f"\n\nIMAGE {i+1} DESCRIPTION:\n{description}"})
                    else:
                        # Determine MIME type
                        mime_type = self._detect_image_mime_type(image_data)
                        parts.append({
                            "inline_data": {
                                "mime_type": mime_type,
                                "data": image_data
                            }
                        })
                        # Add caption if available
                        caption = chunk.metadata.get('caption', '')
                        if caption:
                            parts.append({"text": f"\nImage {i+1} caption: {caption}"})
                
                except Exception as e:
                    logger.warning(f"Failed to process image chunk {i+1}: {e}")
                    # Fallback to text description
                    description = chunk.metadata.get('caption', 'Image processing failed')
                    parts.append({"text": f"\n\nIMAGE {i+1} DESCRIPTION:\n{description}"})
        
        contents = [{"parts": parts}]
        return contents

    def _is_valid_base64_image(self, data: str) -> bool:
        """
        Validate if the provided string is valid base64 image data.

        Args:
            data (str): Base64 string to validate

        Returns:
            bool: True if valid base64 image data
        """
        try:
            # Try to decode base64
            decoded = base64.b64decode(data)
            # Check if it starts with common image headers
            image_headers = [
                b'\xff\xd8\xff',  # JPEG
                b'\x89PNG\r\n\x1a\n',  # PNG
                b'GIF87a',  # GIF87a
                b'GIF89a',  # GIF89a
                b'RIFF'  # WebP (starts with RIFF)
            ]
            return any(decoded.startswith(header) for header in image_headers)
        except Exception:
            return False

    def _detect_image_mime_type(self, base64_data: str) -> str:
        """
        Detect MIME type of base64 image data.

        Args:
            base64_data (str): Base64 encoded image data

        Returns:
            str: MIME type of the image
        """
        try:
            decoded = base64.b64decode(base64_data[:100])  # Check first bytes
            
            if decoded.startswith(b'\xff\xd8\xff'):
                return "image/jpeg"
            elif decoded.startswith(b'\x89PNG\r\n\x1a\n'):
                return "image/png"
            elif decoded.startswith(b'GIF87a') or decoded.startswith(b'GIF89a'):
                return "image/gif"
            elif decoded.startswith(b'RIFF') and b'WEBP' in decoded[:20]:
                return "image/webp"
            else:
                return "image/jpeg"  # Default fallback
        except Exception:
            return "image/jpeg"  # Default fallback

    def _prepare_messages(self, chunks: List[LLMChunk], prompt: str) -> List[Dict[str, Any]]:
        """
        This method is required by the base class but not used in Gemini.
        Gemini uses _prepare_contents instead.
        """
        # This is here for interface compatibility
        # Gemini uses _prepare_contents method instead
        raise NotImplementedError("Gemini uses _prepare_contents method instead of _prepare_messages")